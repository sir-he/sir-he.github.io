---
layout: post
keyword: "sir-he,sir-he blog,,php处理大日志文件,利用程序获取日志中请求url"
excerpt: "利用程序获取日志中请求url"
title:  "利用程序获取日志中请求url"
date:   2017-10-13 20:29:06
tags:   [snorkel, lagos, portugal]
---

今天需要整理日志请求过来的url

记录一下开始到结束的过程吧

1、日志文件过大，需要分段，下面写了一个php脚本

    <?php
        $i = 0; //分割的块编号
        $fp = fopen("logstash_20160712.log","rb"); //要分割的文件
        $file = fopen("split_hash.txt","a"); //记录分割的信息的文本文件
        while(!feof($fp))
        {
            $handle = fopen("logstash_{$i}.txt","wb");
            fwrite($handle,fread($fp,40820000)); //40820000可以自定义.就是每个所分割的文件大小
            fwrite($file,"file.{$i}\r\n");
            fclose($handle);
            unset($handle);
            $i++;
        }
        fclose ($fp);
        fclose ($file);
        echo "ok";
    ?>


2、文件已经分割好了，下面需要整理url，每个日志记录的不一样，有的需要正则（后续部分还需要写正则匹配-。- 我们接口响应时间，和请求到别人接口的时间），我直接解的json，然后存到文本

    <?php
        header("Content-type: text/html; charset=utf-8"); 
        $file = 'logstash_1.log';    //取出url的文件
        $myfile = fopen("url.txt", "w") or die("Unable to open file!");
        if(!is_file($file)){
            echo $file.'不存在';die;
        }
        $arr = file($file);
        $result = [];

        foreach($arr as $line) {
            $line = json_decode($line, true);
            fwrite($myfile, $line['category']."\n");
        }
        echo 'ok';
        fclose($myfile);

【注：利用正则匹配的数据是完整的，第三部可以直接忽略掉~~】

3、利用编辑器去重，我用的是sublime，直接按F9排序，然后前面有一部分是空白，ctrl+shift+home，直接选中光标之前的内容，删除。但是后面有好多重复的需要去掉，在网上找了半天，编辑器替换和下载的软件都不好使，气死我了。。。。

4、万般无奈之下只好还是写脚本呗，但是在网上找的都不好使，时间匆忙，只好利用数据库【mysql】的去重了

创建表的sql语句

    CREATE TABLE `logtest` (
      `id` int(11) NOT NULL AUTO_INCREMENT,
      `test` varchar(100) DEFAULT NULL,
      PRIMARY KEY (`id`)
    )

导入文本数据   把多个文件整合起来，cmd进入文件目录，运行 --> type url*.txt >>xx.txt

load data local infile "D:/data/url/test1.txt"

into table logtest(test); 

全部文本数据导入之后去重

select distinct \`test\` from logtest
